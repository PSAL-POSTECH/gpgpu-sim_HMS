#This config models the Ampere
#This file name is RTX 3070, but config is modeling A100
# functional simulator specification
-gpgpu_ptx_instruction_classification 0
-gpgpu_ptx_sim_mode 0
-gpgpu_ptx_force_max_capability 86

# Device Limits
-gpgpu_stack_size_limit 1024
-gpgpu_heap_size_limit 8388608
-gpgpu_runtime_sync_depth_limit 2
-gpgpu_runtime_pending_launch_count_limit 2048
-gpgpu_kernel_launch_latency 5000
-gpgpu_TB_launch_latency 0

# Compute Capability
-gpgpu_compute_capability_major 8
-gpgpu_compute_capability_minor 6

# PTX execution-driven
#-gpgpu_ptx_convert_to_ptxplus 0
-gpgpu_ptx_save_converted_ptxplus 0

# high level architecture configuration
-gpgpu_n_clusters 21
-gpgpu_n_cores_per_cluster 1
-gpgpu_n_mem 8
-gpgpu_n_sub_partition_per_mchannel 2

# clock domains
#-gpgpu_clock_domains <Core Clock>:<Interconnect Clock>:<L2 Clock>:<DRAM Clock>
-gpgpu_clock_domains 901:901:901:1000

# shader core pipeline config
-gpgpu_shader_registers 65536
-gpgpu_registers_per_block 65536
-gpgpu_occupancy_sm_number 86

# This implies a maximum of 64 warps/SM
-gpgpu_shader_core_pipeline 2048:32
-gpgpu_shader_cta 32
-gpgpu_simd_model 1

# Pipeline widths and number of FUs
# ID_OC_SP,ID_OC_DP,ID_OC_INT,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_INT,OC_EX_SFU,OC_EX_MEM,EX_WB,ID_OC_TENSOR_CORE,OC_EX_TENSOR_CORE
-gpgpu_pipeline_widths 4,4,4,4,4,4,4,4,4,4,8,4,4
-gpgpu_num_sp_units 4
-gpgpu_num_sfu_units 4
-gpgpu_num_dp_units 4
-gpgpu_num_int_units 4
-gpgpu_tensor_core_avail 1
-gpgpu_num_tensor_core_units 4

# Instruction latencies and initiation intervals
# "ADD,MAX,MUL,MAD,DIV"
# All Div operations are executed on SFU unit
-ptx_opcode_latency_int 4,4,4,4,21
-ptx_opcode_initiation_int 2,2,2,2,2
-ptx_opcode_latency_fp 4,4,4,4,39
-ptx_opcode_initiation_fp 1,1,1,1,2
-ptx_opcode_latency_dp 64,64,64,64,330
-ptx_opcode_initiation_dp 64,64,64,64,130
-ptx_opcode_latency_sfu 21
-ptx_opcode_initiation_sfu 8
-ptx_opcode_latency_tesnor 64
-ptx_opcode_initiation_tensor 64

# sub core model: in which each scheduler has its own register file and EUs
# i.e. schedulers are isolated
-gpgpu_sub_core_model 1
# disable specialized operand collectors and use generic operand collectors instead
-gpgpu_enable_specialized_operand_collector 0
-gpgpu_operand_collector_num_units_gen 8
-gpgpu_operand_collector_num_in_ports_gen 8
-gpgpu_operand_collector_num_out_ports_gen 8
# register banks
-gpgpu_num_reg_banks 16
-gpgpu_reg_file_port_throughput 2

# warp scheduling
-gpgpu_num_sched_per_core 4
-gpgpu_scheduler lrr
# a warp scheduler issue mode
-gpgpu_max_insn_issue_per_warp 1
-gpgpu_dual_issue_diff_exec_units 1

## L1/shared memory configuration
# <nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>:**<fifo_entry>
# ** Optional parameter - Required when mshr_type==Texture Fifo
# In adaptive cache, we adaptively assign the remaining shared memory to L1 cache 
# For more info, see https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-7-x 
-gpgpu_adaptive_cache_config 1
-gpgpu_shmem_option 0,8,16,32,64,128
-gpgpu_unified_l1d_size 192
# L1 cache configuration
-gpgpu_l1_banks 4
-gpgpu_cache:dl1 S:4:128:128,L:T:m:L:L,A:384:48,16:0,32
-gpgpu_l1_latency 15
-gpgpu_gmem_skip_L1D 0
-gpgpu_flush_l1_cache 1
-gpgpu_n_cluster_ejection_buffer_size 32
-gpgpu_l1_cache_write_ratio 25

# shared memory  configuration
-gpgpu_shmem_size 131072
-gpgpu_shmem_sizeDefault 131072
-gpgpu_shmem_per_block 65536
-gpgpu_smem_latency 20
# shared memory bankconflict detection 
-gpgpu_shmem_num_banks 32
-gpgpu_shmem_limited_broadcast 0
-gpgpu_shmem_warp_parts 1
-gpgpu_coalesce_arch 86

# L2 cache - 512 KB per memory sub partition (1024 KB per memory partition)
-gpgpu_cache:dl2 S:256:128:16,L:B:m:L:P,A:192:4,32:0,32
-gpgpu_cache:dl2_texture_only 0
-gpgpu_dram_partition_queues 64:64:64:64
-gpgpu_perf_sim_memcpy 1
-gpgpu_memory_partition_indexing 2

# 128 KB Inst.
-gpgpu_cache:il1 N:64:128:16,L:R:f:N:L,S:2:48,4
-gpgpu_inst_fetch_throughput 4
# 128 KB Tex
# Note, TEX is deprected since Volta, It is used for legacy apps only. Use L1D cache instead with .nc modifier or __ldg mehtod
-gpgpu_tex_cache:l1 N:4:128:256,L:R:m:N:L,T:512:8,128:2
# 64 KB Const
-gpgpu_const_cache:l1 N:128:64:8,L:R:f:N:L,S:2:64,4
-gpgpu_perfect_inst_const_cache 1

# interconnection
# use built-in local xbar
-network_mode 2
-icnt_in_buffer_limit 512
-icnt_out_buffer_limit 512
-icnt_subnets 2
-icnt_flit_size 40
-icnt_arbiter_algo 1

# memory partition latency config 
-gpgpu_l2_rop_latency 120
-dram_latency 100

# dram model config
-gpgpu_dram_scheduler 1
-gpgpu_frfcfs_dram_sched_queue_size 128
-gpgpu_dram_return_queue_size 192

# for HBM, three stacks, 24 channles, each (128 bits) 16 bytes width
-gpgpu_n_mem_per_ctrlr 1
-gpgpu_dram_buswidth 16
-gpgpu_dram_burst_length 2
-dram_data_command_freq_ratio 2  # HBM is DDR
-gpgpu_mem_address_mask 1
-gpgpu_mem_addr_mapping dramid@8;RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RRRRRRRR.RBBBCCCB.CCCSSSSS

# HBM timing are adopted from hynix JESD235 standered and nVidia HPCA 2017 paper (http://www.cs.utah.edu/~nil/pubs/hpca17.pdf)
# Timing for 1 GHZ
# tRRDl and tWTR are missing, need to be added
#-gpgpu_dram_timing_opt "nbk=16:CCD=1:RRD=4:RCD=14:RAS=33:RP=14:RC=47:
#                        CL=14:WL=2:CDLR=3:WR=12:nbkgrp=4:CCDL=2:RTPL=4"

# Timing for 850 MHZ, V100 HBM runs at 850 MHZ
-gpgpu_dram_timing_opt "nbk=16:CCD=1:RRD=3:RCD=12:RAS=28:RP=12:RC=40:
                        CL=12:WL=2:CDLR=3:WR=10:nbkgrp=4:CCDL=2:RTPL=3"

# HBM has dual bus interface, in which it can issue two col and row commands at a time
-dram_dual_bus_interface 1
# select lower bits for bnkgrp to increase bnkgrp parallelism
-dram_bnk_indexing_policy 0
-dram_bnkgrp_indexing_policy 1

#-dram_seperate_write_queue_enable 1
#-dram_write_queue_size 64:56:32

# stat collection
-gpgpu_memlatency_stat 14 
-gpgpu_runtime_stat 500
-enable_ptx_file_line_stats 1
-visualizer_enabled 0

# tracing functionality
#-trace_enabled 1
#-trace_components WARP_SCHEDULER,SCOREBOARD
#-trace_sampling_core 0


### items for functional and timing simulation of UVM ###

# gddr size should be less than or equal to 1GB, in the unit of MB/GB
-gddr_size 1GB
#-dram_size
#-scm_size

# number of tlb entries per SM
-tlb_size 4096

# average page table walk latency (in core cycle)
# for 4K page, set to 100 and for 2M page, set to 66
-page_table_walk_latency 100

# page eviction policy
# 0 - lru 2MB (default)
# 1 - lru tree-based neighborhood
# 2 - lru sequential locality 64K
# 3 - random 4KB
# 4 - LFU 2MB
# 5 - lru 4KB
-eviction_policy 1

# invalidate clean pages directly instead of writing back
-invalidate_clean 0

# reserve percentage (e.g. 10 or 20) of accesses pages from eviction in hope that they will be accessed in next iteration
-reserve_accessed_page_percent 0

# percentage of free page buffer to trigger the page eviction (e.g. 5 or 10)
-percentage_of_free_page_buffer 0

# pcie bandwidth per direction
-pcie_bandwidth 6.4GB/s

# enable/disable GMMU statistics profiling for UVM
-sim_prof_enable 0

# disable deadlock check for UVM
-gpgpu_deadlock_detect 0

# hardware prefetcher
# 0 - disabled
# 1 - tree-based neighborhood (default)
# 2 - sequential locality 64K
# 3 - random 4 K
-hardware_prefetch 1

# hardware prefetcher under over-subscription
# 0 - disable upon eviction (default)
# 1 - tree-based neighborhood
# 2 - sequential locality 64K
# 3 - random 4 K
-hwprefetch_oversub 1

# latency in core cycle to handle page fault (45us)
# encompass the overhead of stalling threads, deciding memory address, page table walk, maintaining page flags, transfer chunks and orders
-page_fault_latency 18020

# enabling accurate simulation for stalling warps and serializing accesses for page fault handling (default 0)
-enable_accurate_simulation 0

# Enable direct CPU-memory access from GPU
# 0 - disable
# 1 - adaptive
# 2 - always
# 3 - after oversubscription
-enable_dma 0

# Access counter threshold for migrating the page from cpu to gpu
-migrate_threshold 8

# Oversubscription Multiplicative Penalty Factor for Adaptive DMA
-multiply_dma_penalty 2

# enabling access pattern detection, policy engine, and adaptive memory management
-enable_smart_runtime 0



-use_ramulator 1
-ramulator_memory_type HMS
-update_pte_frequency 128
-enable_ctrl_printer 1
-enable_stat_printer 1
-first_in_gpu 0
-dram_cache_policy ALWAYS
-dram_cache_metadata_level 4
-power_simulation_enabled 1
-accelwattch_xml_file accelwattch_sass_sim.xml
-enable_tag_cache 0
-num_tag_bits 4
-tag_arr_on_chip_percent 2
-assoc_of_dram_cache 1
-num_tag_arr_assocs 16
-tag_arr_line_size 32
-tag_arr_evict_buffer_size 0
-tag_arr_queue_size 128
-tag_cache_latency 1
-continuous 0

